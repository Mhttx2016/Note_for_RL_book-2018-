\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Reinforcement\040Learning}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Elements\040of\040RL}{chapter.1}% 3
\BOOKMARK [-1][-]{part.1}{I\040Tabular\040Solution\040Methods}{}% 4
\BOOKMARK [0][-]{chapter.2}{Multi-armed\040Bandits}{part.1}% 5
\BOOKMARK [1][-]{section.2.1}{A\040k-armed\040Bandit\040Problem}{chapter.2}% 6
\BOOKMARK [1][-]{section.2.2}{Action-value\040Methods}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.3}{The\04010-armed\040Testbed}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.4}{Incremental\040Implementation}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.5}{Tracking\040a\040Nonstationary\040Problem}{chapter.2}% 10
\BOOKMARK [1][-]{section.2.6}{Optimistic\040Initial\040Values}{chapter.2}% 11
\BOOKMARK [1][-]{section.2.7}{Upper-Confidence-Bound\040Action\040Selection}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.8}{Gradient\040Bandit\040Algorithms}{chapter.2}% 13
\BOOKMARK [0][-]{chapter.3}{Finit\040Markov\040Decision\040Process}{part.1}% 14
\BOOKMARK [1][-]{section.3.1}{The\040Agent-Environment\040Interface}{chapter.3}% 15
\BOOKMARK [1][-]{section.3.2}{Goals\040and\040Rewards}{chapter.3}% 16
\BOOKMARK [1][-]{section.3.3}{Returns\040and\040Episodes}{chapter.3}% 17
\BOOKMARK [1][-]{section.3.4}{Unified\040Notation\040for\040Episodic\040and\040Continuing\040Tasks}{chapter.3}% 18
\BOOKMARK [1][-]{section.3.5}{Policies\040and\040Value\040Functions}{chapter.3}% 19
\BOOKMARK [1][-]{section.3.6}{Optimal\040Policies\040and\040Optimal\040Value\040Functions}{chapter.3}% 20
\BOOKMARK [0][-]{chapter.4}{Dynamic\040Programming}{part.1}% 21
\BOOKMARK [1][-]{section.4.1}{Policy\040Evaluation\040\(Prediction\)}{chapter.4}% 22
\BOOKMARK [1][-]{section.4.2}{Policy\040Improvement}{chapter.4}% 23
\BOOKMARK [1][-]{section.4.3}{Policy\040Iteration}{chapter.4}% 24
\BOOKMARK [1][-]{section.4.4}{Value\040Iteration}{chapter.4}% 25
\BOOKMARK [1][-]{section.4.5}{Asynchronous\040Dynamic\040Programming}{chapter.4}% 26
\BOOKMARK [1][-]{section.4.6}{Generalized\040Policy\040Iteration}{chapter.4}% 27
\BOOKMARK [1][-]{section.4.7}{Efficiency\040of\040Dynamic\040Programming}{chapter.4}% 28
\BOOKMARK [0][-]{chapter.5}{Monte\040Carlo\040Methods}{part.1}% 29
\BOOKMARK [1][-]{section.5.1}{Monte\040Carlo\040Prediction}{chapter.5}% 30
\BOOKMARK [1][-]{section.5.2}{Monte\040Carlo\040Estimation\040of\040Action\040Values}{chapter.5}% 31
\BOOKMARK [1][-]{section.5.3}{Monte\040Carlo\040Control\(Approximate\040Optimal\040Policies\)}{chapter.5}% 32
\BOOKMARK [1][-]{section.5.4}{Monte\040Carlo\040Control\040without\040Exploring\040Starts}{chapter.5}% 33
\BOOKMARK [1][-]{section.5.5}{Off-policy\040Prediction\040via\040Importance\040Sampling}{chapter.5}% 34
\BOOKMARK [-1][-]{part.2}{II\040Approximate\040Solution\040Methods}{}% 35
