\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}
\contentsline {section}{\numberline {1.1}Reinforcement Learning}{5}{section.1.1}
\contentsline {section}{\numberline {1.2}Elements of RL}{5}{section.1.2}
\contentsline {part}{I\hspace {1em}Tabular Solution Methods}{7}{part.1}
\contentsline {chapter}{\numberline {2}Multi-armed Bandits}{9}{chapter.2}
\contentsline {section}{\numberline {2.1}A k-armed Bandit Problem}{9}{section.2.1}
\contentsline {section}{\numberline {2.2}Action-value Methods}{9}{section.2.2}
\contentsline {section}{\numberline {2.3}The 10-armed Testbed}{9}{section.2.3}
\contentsline {section}{\numberline {2.4}Incremental Implementation}{9}{section.2.4}
\contentsline {section}{\numberline {2.5}Tracking a Nonstationary Problem}{10}{section.2.5}
\contentsline {section}{\numberline {2.6}Optimistic Initial Values}{11}{section.2.6}
\contentsline {section}{\numberline {2.7}Upper-Confidence-Bound Action Selection}{11}{section.2.7}
\contentsline {section}{\numberline {2.8}Gradient Bandit Algorithms}{11}{section.2.8}
\contentsline {chapter}{\numberline {3}Finit Markov Decision Process}{13}{chapter.3}
\contentsline {section}{\numberline {3.1}The Agent-Environment Interface}{13}{section.3.1}
\contentsline {section}{\numberline {3.2}Goals and Rewards}{13}{section.3.2}
\contentsline {section}{\numberline {3.3}Returns and Episodes}{13}{section.3.3}
\contentsline {section}{\numberline {3.4}Unified Notation for Episodic and Continuing Tasks}{14}{section.3.4}
\contentsline {section}{\numberline {3.5}Policies and Value Functions}{14}{section.3.5}
\contentsline {section}{\numberline {3.6}Optimal Policies and Optimal Value Functions}{15}{section.3.6}
\contentsline {chapter}{\numberline {4}Dynamic Programming}{17}{chapter.4}
\contentsline {section}{\numberline {4.1}Policy Evaluation (Prediction)}{17}{section.4.1}
\contentsline {section}{\numberline {4.2}Policy Improvement}{18}{section.4.2}
\contentsline {section}{\numberline {4.3}Policy Iteration}{19}{section.4.3}
\contentsline {section}{\numberline {4.4}Value Iteration}{19}{section.4.4}
\contentsline {section}{\numberline {4.5}Asynchronous Dynamic Programming}{20}{section.4.5}
\contentsline {section}{\numberline {4.6}Generalized Policy Iteration}{20}{section.4.6}
\contentsline {section}{\numberline {4.7}Efficiency of Dynamic Programming}{20}{section.4.7}
\contentsline {chapter}{\numberline {5}Monte Carlo Methods}{21}{chapter.5}
\contentsline {section}{\numberline {5.1}Monte Carlo Prediction}{21}{section.5.1}
\contentsline {section}{\numberline {5.2}Monte Carlo Estimation of Action Values}{21}{section.5.2}
\contentsline {section}{\numberline {5.3}Monte Carlo Control(Approximate Optimal Policies)}{22}{section.5.3}
\contentsline {section}{\numberline {5.4}Monte Carlo Control without Exploring Starts}{23}{section.5.4}
\contentsline {section}{\numberline {5.5}Off-policy Prediction via Importance Sampling}{23}{section.5.5}
\contentsline {part}{II\hspace {1em}Approximate Solution Methods}{25}{part.2}
