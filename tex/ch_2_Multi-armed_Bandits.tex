%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../main.tex
% !TEX encoding = UTF-8 Unicode
\chapter{Multi-armed Bandits}
\label{chap:ch_2_Multi-armed_Bandits}

% \section{Multi-armed Bandit Problem}
\section{A k-armed Bandit Problem}
We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected:
\begin{equation}
        q_*(a) \dot{=} \mathbb{E}[R_t|A_t=a]
\end{equation}
We denote the estimated value of action a at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.
\section{Action-value Methods}
\textbf{sample-average} method: estimates $q_*(a)$ by averaging the rewards actually received:
\begin{equation}
    Q_t(a) \dot{=} \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbbm{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbbm{1}_{A_i=a}}
\end{equation}
As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$.

\textbf{greedy action selection:} 
\begin{equation}
    A_t \dot{=} arg\max_{a}Q_t(a)
\end{equation}

\textbf{$\epsilon$-greedy action selection:}
\begin{equation}
    A_t \dot{=} \begin{cases}
        arg \max_a{Q_t(a)}, \quad with \quad probability \quad 1-\epsilon \\
        a \quad random \quad action, \quad with \quad probability \quad \epsilon 

\end{cases}
\end{equation}
In the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to $q_*(a)$.

\section{The 10-armed Testbed}

\section{Incremental Implementation}
\begin{equation}
    Q_n \dot{=} \frac{R_1+R_2+\dots+R_{n-1}}{n-1}
\end{equation}

\begin{equation}
\label{eq:update_rule_aveage}
\begin{split}
    Q_{n+1} = & \frac{1}{n}\sum_{i=1}^{n}R_i \\
    =&\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i)\\
    =&\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i)\\
    =&\frac{1}{n}(R_n+(n-1)Q_n)\\
    =&Q_n + \frac{1}{n}(R_n-Q_n)
\end{split}
\end{equation}
The general form of \ref{eq:update_rule_aveage} is:
\begin{equation}
    NewEstimate = OldEstimate + StepSize * [Target - OldEstimate]
\end{equation}
Note that the step-size parameter (StepSize, denote as $\alpha$ or $\alpha_t(a)$) used in the incremental method described above changes from time step to time step, here $\alpha_t(a)=\frac{1}{n}$.

\section{Tracking a Nonstationary Problem}
The averaging methods discussed so far are appropriate for \textbf{stationary bandit problems}, that is, for
bandit problems in which the reward probabilities do not change over time. \textbf{In effectively nonstationary cases it makes sense to give more weight to recent rewards than to long-past rewards.} One of the most popular ways
of doing this is to use a \textbf{constant step-size parameter}:
\begin{equation}
    Q_{n+1} \dot{=} Q_n+\alpha[R_n-Q_n]
\end{equation}
This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:
\begin{equation}
\begin{split}
    Q_{n+1} = & Q_{n} + \alpha[R_n-Q_n]\\
            = & \alpha R_n + (1-\alpha)Q_n\\
            = & \alpha R_n + (1-\alpha)[\alpha R_{n-1}+(1-\alpha) Q_{n-1}]\\
            = & \alpha R_n + (1-\alpha)\alpha R_{n-1} +(1-\alpha)^2 Q_{n-1}\\
            = & \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 \alpha R_{n-2} + \\
                            & \cdots+(1-\alpha)^{n-1}\alpha R_1 + (1-\alpha)^n Q_1\\
            = & (1-\alpha)^nQ_1+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_i
\end{split}
\end{equation}
Note that the weight, $\alpha(1-\alpha)^{n-i}$, given to the reward $R_i$ depends on how many rewards ago, Accordingly, this is sometimes called an \textbf{exponential recency-weighted average}.
eros
Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n_th$ selection of action $a$. Conditions required to assure convergence with probability 1:
\begin{equation}
\sum_{n=1}^{\infty} \alpha_n(a) = \infty  \quad and \quad \sum_{n=1}^{\infty} \alpha_n^2 \ < \infty
\end{equation}

The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random fluctuations. The second condition guarantees that eventually the steps become small enough to assure convergence.

Note that both convergence conditions are met for the sample-average case, $\alpha_n(a)=\frac{1}{n}$, but not for
the case of constant step-size parameter, $\alpha_n(a)=\alpha$. In the latter case, the second condition is not met, indicating that \textbf{the estimates never completely converge but continue to vary in response to the most recently received rewards. As we mentioned above, this is actually desirable in a nonstationary environment.}

\section{Optimistic Initial Values}
All the methods we have discussed so far are dependent to some extent on the initial action-value
estimates, $Q_1(a)$. These methods are biased by their initial estimates. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to \textbf{supply some prior knowledge}
about what level of rewards can be expected.

Initial action values can also be used as a simple way to encourage exploration. We call this technique for encouraging exploration \textbf{optimistic initial values}. It is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much.

\section{Upper-Confidence-Bound Action Selection}
It would be better to select among the non-greedy actions according to their \textbf{potential for actually being optimal},
taking into account both \textbf{how close their estimates are to being maximal and the uncertainties in those
estimates}. 

\begin{equation}
    A_t \dot{=} arg \max_{a}[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}]
\end{equation}

The idea of this \textbf{upper confidence bound (UCB)} action selection is that the square-root term is a
measure of \textbf{the uncertainty or variance in the estimate of $a$ ’s value.} The quantity being max’ed over is
thus a sort of upper bound on the possible true value of action $a$, with $c$ determining the confidence level.

One difficulty is in dealing with nonstationary problems. Another difficulty is dealing with large state spaces, particularly when using function approximation.

\section{Gradient Bandit Algorithms}
$H_t(a)$ as a numerical preference of each action $a$. 
\begin{equation}
    \pi_t(a) \dot{=} softmax(H_t(a))\dot{=}\frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}
\end{equation}

\textbf{The Bandit Gradient Algorithm as Stochastic Gradient Ascent:} In exact gradient ascent, each preference $H_t(a)$ would be incremented proportional to the increment’s effect on performance:
\begin{equation}
\label{eq:bga}
    H_{t+1}(a) \dot{=} H_t(a) + \alpha \frac{\partial\mathbb{E}[R_t]}{\partial H_t(a)}
\end{equation}

\begin{equation}
    \mathbb{E}[R_t] = \sum_{b}\pi_t(b)q_{*}(b)
\end{equation}

\begin{equation}
\begin{split}
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = & \frac{\partial}{\partial H_t(a)}[\sum_{b}\pi_t(b)q_*(b)]\\
    = & \sum_{b}q_*(b) \frac{\partial \pi_t(b)}{\partial H_t(a)}\\
    = & \sum_{b}(q_*(b)-X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\end{split}
\end{equation}
where $X_t$ can be any scalar that does not depend on $b$, we can include it here because the gradient sums to zero over all the actions, $\sum_b\frac{\partial \pi_t(b)}{\partial H_t(a)}=0$.
\begin{equation}
\begin{split}
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = & \sum_b \pi_t(b) (q_*(b)-X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(b)\\
    = & \mathbb{E}[(q_*(A_t)-X_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)] \\
    = & \mathbb{E}[(R_t-\bar{R}_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\end{split}
\end{equation}
we have $\frac{\partial \pi_t(b)}{\partial H_t(a)}=\pi_t(b)(\mathbbm{1}_{a=b}-\pi_t(a))$, and substituted it.
\begin{equation}
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}=\mathbb{E}[(R_t-\bar{R}_t)(\mathbbm{1}_{a=A_t}-\pi_t(a))]
\end{equation}
Substituting a sample of the expectation above for the performance gradient in \ref{eq:bga}:
\begin{equation}
    H_{t+1}(a) = H_t(a) + \alpha (R_t-\bar{R}_t)(\mathbbm{1}_{a=A_t}-\pi_t(a))
\end{equation}