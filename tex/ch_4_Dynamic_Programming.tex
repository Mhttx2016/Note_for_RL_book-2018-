%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../main.tex
% !TEX encoding = UTF-8 Unicode

\chapter{Dynamic Programming}
\label{chap:ch_4_Dynamic_Programming}
The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.

\section{Policy Evaluation (Prediction)}
\begin{enumerate}
	\item \textbf{policy evalution(prediction problem):} compute the state-value function $v_{\pi}$for an arbitrary policy $\pi$.
	\item \textbf{iterative policy evaluation:} The existence and uniqueness of $v_{\pi}$ are guaranteed as long as either $\gamma < 1$ or eventual termination is guaranteed from all states under the policy $\pi$.
	\begin{equation}
		v_{\pi}(s) \dot{=} \sum_{a}\pi(a|s)\sum_{s',r}p(s', r|s,a)[r + \gamma v_{\pi}(s')]
	\end{equation}

	\textbf{Update rule:}
	\begin{equation}
	\label{eq:policy_evaluation}
		v_{k+1}(s) \dot{=} \sum_{a}\pi(a|s)\sum_{s',r}p(s', r|s,a)[r + \gamma v_{k}(s')]
	\end{equation}
	Indeed, the sequence $\{v_k\}$ can be shown in general to converge to $v_{\pi}$ as $k \rightarrow \infty$ under the same conditions that guarantee the existence of $v_{\pi}$.

	\item \textbf{expected update:}All the updates done in DP algorithms are called expected updates because they are based on an expectation over all possible next states rather than on a sample next state.

	\item \textbf{In-place iterative policy evaluation:} With two arrays for $v_k(s)$ and $v_{k+1}(s)$ respectively, the new values can be computed one by one from the old values without the old values being changed. Of course it is easier to use one array and update the values “in place,” that is, with each new value immediately overwriting the old one. show in Algorithm-\ref{alg:iterative_policy_evaluation}.
\end{enumerate}

\begin{algorithm}[H]
\label{alg:iterative_policy_evaluation}
\caption{Iterative Policy Evaluation(In-place)}
\begin{algorithmic}[1]
\Require the policy to be evaluated, a small threshold $\theta > 0$ determining accuracy of estimation, Initialize $V(s)$, for all $s \in \mathcal{S}^+$, arbitrarily except that $V(terminal)$=0.
\Statex
\Repeat
	\State \begin{math}\Delta \leftarrow 0\end{math}
	\For{\begin{math} s \in \mathcal{S}\end{math}}
		\State \begin{math}v \leftarrow V(s)\end{math}
		\State \begin{math}V(s) \leftarrow \sum_a \pi(a|s)\sum_{s',r}p(s', r|s, a)[r+\gamma V(s')]\end{math}
		\State \begin{math}\Delta \leftarrow \max(\Delta , |v-V(s)|)\end{math}
	\EndFor
\Until{\begin{math} \Delta < \theta \end{math}}
\end{algorithmic}
\end{algorithm}

\section{Policy Improvement}
\begin{enumerate}
	\item \textbf{policy improvement theorem:} Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, $\forall s \in \mathcal{S}$,
	\begin{equation}
		q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)
	\end{equation}
	Then the policy $\pi'$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s \in \mathcal{S}$.
	\begin{equation}
		v_{\pi'}(s) \geq v_{\pi}(s)
	\end{equation}

	\item \textbf{policy improvement:} The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy. So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a particular action. It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_{\pi}(s,a)$. In other words, to consider the new greedy policy, $\pi'$, given by
	\begin{equation}
	\label{eq:policy_improvement}
	\begin{split}
		\pi'(s) \dot{=} & arg \max_{a}q_{\pi}(s,a)\\
			= & arg \max_{a} \mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s, A_t=a]\\
			= & arg \max_{a} \sum_{s', r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]
	\end{split}
	\end{equation}

	\item \textbf{optimality:} Suppose the new greedy policy, $\pi'$, is as good as, but not better than, the old policy $\pi$. Then $v_{\pi}=v_{\pi'}$, and from \ref{eq:policy_improvement} it follows that for all $s \in \mathcal{S}$:
	\begin{equation}
	\begin{split}
		v_{\pi'}(s) = & \max_{a} \mathbb{E}[R_{t+1}+ \gamma v_{\pi'}(S_{t+1})|S_t=s, A_t=a]\\
			= & \max_{a} \sum_{s', r}p(s',r|s, a)[r+\gamma v_{\pi'}(s')]
	\end{split}
	\end{equation}
	this is the same as the Bellman optimality equation \ref{eq:bellman_optimality_equation}, and $v_{\pi'}$ must be $v_{*}$, and both $\pi$ and $\pi'$ must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal.

	\item \textbf{stochastic case:} the ideas of this deterministic case extend easily to stochastic policies.
\end{enumerate}

\section{Policy Iteration}
\begin{algorithm}[H]
\label{alg:policy_iteration}
\caption{Policy Iteration(using iterative evaluation) for estimating $\pi \approx \pi_*$}
\begin{algorithmic}[1]
\Procedure{Initialization}{}
	\State $V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A}(s)$ arbitrarily $\forall s \in \mathcal{S}$
\EndProcedure
\Statex
\Procedure{Policy Evaluation}{}
\label{proc:policy_evaluation}
\Repeat
	\State $\Delta \leftarrow 0$
	\For{\begin{math}s \in \mathcal{S}\end{math}(}
			\State $v \leftarrow V(s))$
		\State $V(s) \leftarrow \sum_{s',r}p(s',r|s, \pi(s))[r+\gamma V(s')]$
		\State $\Delta \leftarrow \max(\Delta, |v-V(s)|)$
	\EndFor
\Until{\begin{math} \Delta < \theta \end{math}}
\EndProcedure

\Statex
\Procedure{Policy Improvement}{}
\State $policy\_stable \leftarrow true$
\For{\begin{math}s \in \mathcal{S}\end{math}}
\State $old_action \leftarrow \pi(s)$
\State $\pi(s) \leftarrow arg \max_{a} \sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$
\If{\begin{math}old\_action \neq \pi(s)\end{math}}
	\State $policy\_stable \leftarrow false$
\EndIf
\EndFor
\If{\begin{math} policy\_stable \end{math}}
	\State stop and return $V \approx v_*$ and $\pi \approx \pi_*$
\Else
	\State go to \textbf{procedure} \textsc{Policy Evaluation}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Value Iteration}
In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state).This algorithm is called value iteration, show in Algorithm-\ref{alg:iterative_value_iteration}. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps:
\begin{equation}
\begin{split}
	v_{k+1}(s) \dot{=} & \max_a \mathbb{E}[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a] \\
		= & \max_{a} \sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]
\end{split}
\end{equation}
for all $s \in \mathcal{S}$ . For arbitrary $v_0$ , the sequence ${v_k}$ can be shown to converge to $v_*$ under
the same conditions that guarantee the existence of $V_*$.

Another way of understanding value iteration is by reference to the Bellman optimality
equation \ref{eq:bellman_optimality_equation}, Also note how the value iteration update is
identical to the policy evaluation update \ref{eq:policy_evaluation} except that it requires the maximum to be
taken over all actions.

\begin{algorithm}[H]
\label{alg:iterative_value_iteration}
\caption{Value Iteration, for estimating $\pi \approx \pi_*$}
\begin{algorithmic}[1]
\Require the policy to be evaluated, a small threshold $\theta > 0$ determining accuracy of estimation, Initialize $V(s)$, for all $s \in \mathcal{S}^+$, arbitrarily except that $V(terminal)$=0.
\Statex
\Repeat
	\State \begin{math}\Delta \leftarrow 0\end{math}
	\For{\begin{math} s \in \mathcal{S}\end{math}}
		\State \begin{math}v \leftarrow V(s)\end{math}
		\State \begin{math}V(s) \leftarrow \max_a \sum_{s',r}p(s', r|s, a)[r+\gamma V(s')]\end{math}
		\State \begin{math}\Delta \leftarrow \max(\Delta , |v-V(s)|)\end{math}
	\EndFor
\Until{\begin{math} \Delta < \theta \end{math}}
\Statex
\State Output a deterministic policy, $\pi \approx \pi_*$, such that
$$\pi(s)=arg \max_a \sum_{s',r}p(s',r|s, a)[r+\gamma V(s')]$$
\end{algorithmic}
\end{algorithm}

\section{Asynchronous Dynamic Programming}
A major drawback to the DP methods that we have discussed so far is that they involve
operations over the entire state set of the MDP, that is, they require sweeps of the state
set. \textbf{Asynchronous DP algorithms} are in-place iterative DP algorithms that are not organized
in terms of systematic sweeps of the state set. If $0 \leq \gamma < 1$, asymptotic convergence to $v_*$ is guaranteed given only that all states occur in the sequence $\{s_k\}$ an infinite number of  times.

\section{Generalized Policy Iteration}
Letting policy-evaluation and policy improvement processes interact, independent of the granularity
and other details of the two processes. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value
function always being driven toward the value function for the policy. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal.

\section{Efficiency of Dynamic Programming}
DP methods take to find an optimal policy is polynomial in the number of states and actions.DP is sometimes thought to be of limited applicability because of the \textbf{curse of dimensionality}, the fact that the number of states often grows exponentially with the number of state variables. In fact, DP is comparatively better
suited to handling large state spaces than competing methods such as direct search and linear programming.