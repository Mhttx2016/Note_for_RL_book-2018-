%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../main.tex
% !TEX encoding = UTF-8 Unicode

\chapter{Monte Carlo Methods}
\label{chap:ch_5_Monte_Carlo_Methods}
\section{Monte Carlo Prediction}
\begin{enumerate}
	\item \textbf{Monte Carlo methods:} Estimate value function(expected return) from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value.

	\item \textbf{First-vist MC method vs every-visit MC method:}  The first-visit MC method, as show in Algorithm-\ref{alg:first-visit_mc_prediction}, estimates $v_{\pi}(s)$ as the average of the returns following \textbf{first visits} to $s$ , whereas the every-visit MC method averages the returns following \textbf{all visits} to $s$. Both first-visit MC and every-visit MC converge to $v_{\pi}(s)$ as the number of visits (or first visits) to $s$ goes to infinity.

	\item \textbf{Property of MC methods:} 
	\begin{enumerate}
	\item The ability of Monte Carlo methods to work with sample episodes alone can be a significant advantage when one has no knowledge or even complete knowledge of the environment’s dynamics;

	\item Monte Carlo methods \textbf{do not bootstrap}, the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as is the case in DP;

	\item The computational expense of estimating the value of a single state is independent of the number of states. 
	\end{enumerate}
\end{enumerate}

\begin{algorithm}[H]
\label{alg:first-visit_mc_prediction}
\caption{First-visit MC prediction, for estimating $V \approx v_{\pi}$}
\begin{algorithmic}[1]
\Require the policy $\pi$ to be evaluated, Initialize $V(s)$, arbitarily, for all $s \in \mathcal{S}$, $Returns(s) \leftarrow $ an empty list, for all $s \in \mathcal{S}$
\Statex
\Loop \Comment{for each episode}
	\State Generate an episode following $\pi$:$S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, A_{T-1}, R_T$
	\State $G \leftarrow 0$
	\For{each step of episode, \begin{math}t=T-1, T-2, \dots ,0\end{math}}
		\State $G \leftarrow \gamma G + R_{t+1}$
		\If{\begin{math}S_t\end{math} not in \begin{math}S_0, S_1, \dots, S_{t-1}\end{math}} \Comment{appear in episode for teh first time}
			\State Append $G$ to $Returns(S_t)$
			\State $V(S_t) \leftarrow average(Returns(S_t))$
		\EndIf
	\EndFor
\EndLoop
\end{algorithmic}
\end{algorithm}

\section{Monte Carlo Estimation of Action Values}
\begin{enumerate}
	\item \textbf{Necessity of estimation of action values:} Without a model, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy.

	\item \textbf{First-visit vs ervery visit MC method:} The every-visit MC method estimates the value of a state–action pair as the average of the returns that have followed all the visits to it. The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected.

	\item \textbf{Maintainig exploration:}  If $\pi$is a \textbf{deterministic policy}, then in following $\pi$ one will observe returns only for one of the actions from each state. However, the purpose of learning action values is to help in choosing among the actions available in each state. To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor. For policy evaluation to work for action values, we must assure \textbf{continual exploration}.

	\item \textbf{Exploring starts:} One way to do this is by specifying that the episodes start in a state–action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.
\end{enumerate}

\section{Monte Carlo Control(Approximate Optimal Policies)}
\begin{enumerate}
	\item \textbf{Monte Carlo policy iteration:}In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy $\pi_0$ and ending with the optimal policy and optimal action-value function:
	$$\pi_0 \xrightarrow{E} q_{\pi_0} \xrightarrow{I}
	\pi_1 \xrightarrow{E} q_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \dots \xrightarrow{I} \pi_* \xrightarrow{E} q_{\pi_*}$$
	Policy improvement is done by making the policy greedy with respect to the current (action) value function. 
	\begin{equation}
		\pi(s) \dot{=} arg \max_{a} q(s,a)
	\end{equation}
	 The policy improvement theorem then applies to $\pi_k$ and $\pi_{k+1}$ because, for all $s \in \mathcal{S}$,
	 \begin{equation}
	 \begin{split}
	 	q_{\pi_k}(s, \pi_{k+1}(s)) = & q_{\pi_k}(s, arg \max_a q_{\pi_k}(s,a) \\
	 		= & max_a q_{\pi_k}(s,a) \\
	 		\geq & q_{\pi_k}(s, \pi_k(s)) \\
	 		\geq & v_{\pi_k}(s)
	 \end{split}
	 \end{equation}
	 the theorem assures us that each $\pi_{k+1}$ is uniformly better than $\pi_k$ , or just as good as $\pi_k$.

	\item \textbf{Monte Carlo ES:}(Monte Carlo with Exploring Starts)
	
	\begin{algorithm}[H]
	\label{alg:monte_carlo_es}
	\caption{Monte Carlo ES}
	\begin{algorithmic}[1]
	\Require \\Initialize $\pi(s) \in \mathcal{A}(s)$(arbitarily), for all $s \in \mathcal{S}$,\\
	Initialize $Q(s,a) \in \mathbb{R}$(arbitrarily), for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$,\\
	Initialize $Returns(s, a) \leftarrow empty list$, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$.
	\Statex
	\Repeat
		\State Choose \begin{math} S_0 \in \mathcal{S}, A_0 \in \mathcal{A}(S_0) \end{math} randomly such that all paris have probability \begin{math} > 0 \end{math}

		\State Generate an episode from \begin{math}S_0, A_0\end{math}, following \begin{math} \pi: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\end{math}

		\State \begin{math} G \leftarrow 0 \end{math}

		\For{each step of episode, \begin{math}t=T-1, T-2, ...., 0 \end{math}}
			\State \begin{math}G \leftarrow \gamma G + R_{t+1} \end{math}
			\If{The pair \begin{math}S_t,A_t\end{math} not appears in \begin{math}S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}:\end{math}}
				\State Append \begin{math} G \end{math} to \begin{math}Returns(S_t, A_t)\end{math}
				\State \begin{math}Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))\end{math}
				\State \begin{math}\pi(S_t) \leftarrow arg\max_{a}Q(S_t, a) \end{math}
			\EndIf
		\EndFor
	\Until{Converge}
	\end{algorithmic}
	\end{algorithm}
\end{enumerate}

\section{Monte Carlo Control without Exploring Starts}
\begin{enumerate}
	\item \textbf{$\epsilon-soft$ policy:} meaning that $\pi{a|s} > 0$ for all $s \in \mathcal{S}$ and all $a \in \mathcal{A(s)}$, but gradually shifted closer and closer to deterministic optimal policy.

	\item \textbf{On-policy first-visit MC control (for $\epsilon$-soft policies), estimates $\pi \approx \pi_*$}

	\begin{algorithm}[H]
	\label{alg:on_policy_first_visit_MC}
	\caption{On-policy first-visit MC control}
	\begin{algorithmic}[1]
	\Require \\Initialize $\pi \leftarrow$ an arbitrary $\epsilon-soft$ policy\\
	Initialize $Q(s,a) \in \mathbb{R}$ (arbitrarily), for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$\\
	Initialize $Returns(s, a) \leftarrow empty list$, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
	\Statex
	\Repeat \Comment{for each episode}
		
		\State Generate an episode following \begin{math} \pi: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\end{math}
		\State \begin{math} G \leftarrow 0 \end{math}
		\For{each step of episode, \begin{math}t=T-1, T-2, ...., 0 \end{math}}
			\State \begin{math}G \leftarrow \gamma G + R_{t+1} \end{math}
			\If{The pair \begin{math}S_t,A_t\end{math} not appears in \begin{math}S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}:\end{math}}
				\State Append \begin{math} G \end{math} to \begin{math}Returns(S_t, A_t)\end{math}
				\State \begin{math}Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))\end{math}
				\State \begin{math}A^* \leftarrow arg\max_{a}Q(S_t, a) \end{math}
			\EndIf
				\For{each \begin{math} a \in \mathcal{A}(S_t) \end{math}}
					\If{\begin{math}a = A^*\end{math}} 
						\State \begin{math} \pi(a|S_t) \leftarrow 1-\epsilon+\epsilon/|\mathcal{A}(S_t)| \end{math}
					\Else
						\State \begin{math} \pi(a|S_t) \leftarrow \epsilon/|\mathcal{A}(S_t)| \end{math}
					\EndIf
				\EndFor
		\EndFor
	\Until{Converge}
	\end{algorithmic}
	\end{algorithm}
\end{enumerate}

\section{Off-policy Prediction via Importance Sampling}
\begin{enumerate}
	\item \textbf{off-policy learning:} Use two policies, one is the policy to be learned about and that becomes the optimal policy, called \textbf{target policy}, and one that is more exploratory and is used to generate behavior, called \textbf{behavior policy}.

	\item \textbf{assumption of coverage:} Consider target policy $\pi$ and behavior policy $b$, and both policies are fixed and given. In order to use episodes from $b$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken under $b$. That is, we require that $\pi(a|s) > 0$ implies $b(a|s)>0$. It follows from cob=verage that $b$ must be stochastic in states where it is not identical to $\pi$.

	\item \textbf{importance sampling:} a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occuring under target and behavior policies, called the \textbf{importance-sampling ratio}.

	\item \textbf{importance ratio:} given a starting state $S_t$, the probability of subsequent state-action trajectories, $A_t, S_{t+1}, A_{t+1},...,S_T$, occuring under any policy $\pi$ is 
	\begin{equation}
	\begin{split}
	Pr\{A_t, S_{t+1}, A_{t+1},...,S_T|S_t, A_{t:T-1}\sim \pi\} =& \pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\pi(A_{t+1}|S_{t+1})...p(S_T|S_{T-1}, A_{T-1}) \\
	= &\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)
	\end{split}
	\end{equation}
	the relative probability of the trajectory under the target and behavior policies (the importace sampling ratio) is 
	\begin{equation}
	\rho_{t:T-1} \dot{=} \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}
	\end{equation}

	\item \textbf{estimating the expected returns (values) under the target policy:} Get returns $G_t$ due to the behavior policy. $v_b(s) = \mathbb{E}[G_t|S_t=s]$, the ratio $\rho_{t:T-1}$ transforms the $v_b(s)$ to $v_{\pi}(s)$.
	\begin{equation}
	\mathbb{E}[\rho_{t:T-1}G_t|S_t=s]=v_{\pi}(s)
	\end{equation} 

	\item \textbf{ordinary importance sampling:} To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and avrage the results
	\begin{equation}
	V(s) \dot{=} \frac{\sum_{t \in \mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{|\mathcal{J}(s)|}
	\end{equation}

	\item \textbf{weighted importance sampling:}
	\begin{equation}
	V(s) \dot{=} \frac{\sum_{t \in \mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t \in \mathcal{J}(s)\rho_{t:T(t)-1}}}
	\end{equation}

	\item \textbf{difference between the two importance sampling:} for the first-visit methods, ordinary importance sampling is unbiased whereas weighted importance sampling is baised (though the bias convergesasymptotically to zero).On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite. The every-visit methods for ordinary and weighed importance sampling are both biased, though, again, the bias falls asymptotically to zero as the number of samples increases.

\end{enumerate}